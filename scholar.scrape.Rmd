---
title: "scholar.scrape"
author: "Simon Topp"
date: "5/30/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Scholar scrape.

Code taken from Kay Cichini
https://github.com/gimoya/theBioBucket-Archives/blob/master/R/Functions/GScholarScraper_3.2.R

Srapes Google Scholar for publication metadata given certain search terms.

```{r}
# File-Name: GScholarScraper_3.2.R
# Date: 2013-07-11
# Author: Kay Cichini
# Email: kay.cichini@gmail.com
# Purpose: Scrape Google Scholar search result
# Packages used: XML
# Licence: CC BY-SA-NC
#
# Arguments:
# (1) input:
# A search string as used in Google Scholar search dialog
#
# (2) write:
# Logical, should a table be writen to user default directory?
# if TRUE ("T") a CSV-file will be created.
#
# Difference to version 3:
# (3) added "since" argument - define year since when publications should be returned..
# defaults to 1900..
#
# (4) added "citation" argument - logical, if "1" citations are included
# defaults to "0" and no citations will be included..
# added field "YEAR" to output 
#
# Caveat: if a submitted search string gives more than 1000 hits there seem
# to be some problems (I guess I'm being stopped by Google for roboting the site..)
#
# And, there is an issue with this error message:
# > Error in htmlParse(URL): 
# > error in creating parser for http://scholar.google.com/scholar?q
# I haven't figured out his one yet.. most likely also a Google blocking mechanism..
# Reconnecting / new IP-address helps..


GScholar_Scraper <- function(input, since = 1900, write = F, citation = 0) {

    require(XML)
    
    # flip values because the url uses 0 for inclusion of citations 
    citation <- ifelse(citation == 1, 0, 1)

    # putting together the search-URL:
    URL <- 'https://scholar.google.com/scholar?hl=en&scisbd=1&as_sdt=1%2C34&q=%22remote+sensing%22%2C+%22water+quality%22%2C+%22lakes%22&btnG='
    # URL <- paste0("http://scholar.google.com/scholar?q=", input, "&as_sdt=1,5&as_vis=", 
    #              citation, "&as_ylo=", since)
    # cat("\nThe URL used is: ", "\n----\n", paste0("* ", "http://scholar.google.com/scholar?q=", input, "&as_sdt=1,5&as_vis=", 
    #              citation, "&as_ylo=", since, " *"))
    
    # get content and parse it:
    doc <- htmlParse(URL)
    
    # number of hits:
    h1 <- xpathSApply(doc, "//div[@id='gs_ab_md']", xmlValue)
    h2 <- unlist(strsplit(h1, "\\s"))
    # in splitted string it is the second element which contains digits,
    # grab it and remove decimal signs and convert to integer
    num <- as.integer(gsub("[[:punct:]]", "", h2[grep("\\d", h2)[1]]))
    cat("\n\nNumber of hits: ", num, "\n----\n", "If this number is far from the returned results\nsomething might have gone wrong..\n\n", sep = "")
    
    # If there are no results, stop and throw an error message:
    if (num == 0 | is.na(num)) {
        stop("\n\n...There is no result for the submitted search string!")
    }
    
    pages.max <- ceiling(num/20)
    
    # 'start' as used in URL:
    start <- 20 * 1:pages.max - 20
    
    # Collect URLs as list:
    URLs <- paste("http://scholar.google.com/scholar?start=", start, "&q=", input, 
                  "&num=20&as_sdt=1,5&as_vis=", citation, "&as_ylo=", since, sep = "")
    
    scraper_internal <- function(URL) {
        
        doc <- htmlParse(URL, encoding="UTF-8")
        
        # titles:
        tit <- xpathSApply(doc, "//h3[@class='gs_rt']", xmlValue)
        
        # publication:
        pub <- xpathSApply(doc, "//div[@class='gs_a']", xmlValue)
        
        # summaries are truncated, and thus wont be used..  
        # abst <- xpathSApply(doc, '//div[@class='gs_rs']', xmlValue)
        # ..to be extended for individual needs
        options(warn=(-1))
        dat <- data.frame(TITLES = tit, PUBLICATION = pub, 
                          YEAR = as.integer(gsub(".*\\s(\\d{4})\\s.*", "\\1", pub)))
        options(warn=0)
        return(dat)
    }

    result <- do.call("rbind", lapply(URLs, scraper_internal))
    if (write == T) {
      write.table(result, "GScholar_Output.CSV", sep = ";", 
                  row.names = F, quote = F)
      shell.exec("GScholar_Output.CSV") 
      } else {
      return(result)
    }
}

```

## Scrape test

```{r pressure, echo=FALSE}
# EXAMPLES:
# 1:
input <- '"remote sensing" "water quality" "lakes"'
df <- GScholar_Scraper(input, since = 1900, citation = 0)
nrow(df)
hist(df$YEAR, xlab = "Year", 
     main = "Frequency of Publications with\n\"METAPOPULATION\" in Title")

# 2:
input <- "allintitle:life on mars"
GScholar_Scraper(input, since = 2006, citation = 0)

# 3:
input <- "allintitle:ziggy stardust"
GScholar_Scraper(input, write = T)

# 4: ERROR with message:
input <- "allintitle:alien plants restoration"
GScholar_Scraper(input)

# 5: CAVEAT, Google blocks automated requests at about the 1000th hit:
input <- "metapopulation"
df <- GScholar_Scraper(input, since = 1980)
nrow(df)

# 6: this also leads to this error for example no. 1,
# because when including citations (.., citation = 1) 1000 hits are exceeded, 
# Google blocks and dataframe generation is not working..
input <- "intitle:metapopulation"
df <- GScholar_Scraper(input, since = 1980, citation = 1)
```

##Scrape version 2

```{r}
# Author: Miguel Botto Tobar
# Date: 2017-06-19
# Description: This function will retrieve as information as it can about each result on a Google Scholar search page
# Reference: Tony Breyal - https://github.com/tonybreyal/Blog-Reference-Functions/tree/master/R/googleScholarXScraper
# Copyright (c) 2017, under the Creative Commons Attribution-NonCommercial 4.0 Unported (CC BY-NC 4.0) License 
# For more information see: http://creativecommons.org/licenses/by-nc/4.0/
# All rights reserved.

GScholar_Scraper <- function(input, page.ini = 1, pages.max = 0, since = 1900, to = 2017, write = F){
  require(httr)
  require(XML)
  require(RCurl)
  
  # putting together the search-URL:
  URL <- paste0("http://scholar.google.com/scholar?q=",input,"&hl=en&as_sdt=1,5&as_vis=1&as_ylo=",since,"&as_yhi=",to)
  
  cat("\nThe URL used is: ", "\n----\n", paste0("* ","http://scholar.google.com/scholar?q=",input,"&hl=en&as_sdt=1,5&as_vis=1&as_ylo=", since, "&as_yhi=",to," *"))
  
  # get content and parse it:
  doc <- htmlParse(URL)
  
  # number of hits:
  h1 <- xpathSApply(doc, "//div[@id='gs_ab_md']", xmlValue)
  
  h2 <- unlist(strsplit(h1, "\\s"))

  # in splitted string it is the second element which contains digits,
  # grab it and remove decimal signs and convert to integer
  num <- as.integer(gsub("[[:punct:]]", "", h2[grep("\\d", h2)[1]]))
  cat("\n\nNumber of hits: ", num, "\n----\n", "If this number is far from the returned results\nsomething might have gone wrong..\n\n", sep = "")
  # If there are no results, stop and throw an error message:
  if (num == 0 | is.na(num)) {
    stop("\n\n...There is no result for the submitted search string!")
  }
  
  #if (page.ini == 0 | is.na(page.ini)) page.ini <- 1
  if (pages.max == 0 | is.na(pages.max)) pages.max <- ceiling(num/10)
  
  # 'start' as used in URL:
  start <- 10 * page.ini:pages.max - 10
  
  
  # Collect URLs as list:
  #URLs <- paste("http://scholar.google.com/scholar?start=",start,"&q=",input, 
  #              "&btnG=&hl=en&as_sdt=1&as_vis=1&as_ylo=",since,sep="")
  URLs <- paste("http://scholar.google.com/scholar?start=",start,"&q=",input, 
                "&hl=en&as_sdt=1,5&as_vis=1&as_ylo=",since,"&as_yhi=",to,sep="")
  
  GS_xpathSApply <- function(doc, path, FUN) {
    path.base <- "//div[@class='gs_r']"
    
    # get xpaths to each child node of interest
    nodes.len <- length(xpathSApply(doc, path.base))
    paths <- sapply(1:nodes.len, function(i) paste(path.base, "[", i, "]", path, sep = ""))
    
    # extract child nodes
    if (identical(FUN,xmlGetAttr)) {
      xx <- sapply(paths, function(x) xpathSApply(doc, x, FUN, 'href'), USE.NAMES = FALSE)
    } else {
      xx <- sapply(paths, function(x) xpathSApply(doc, x, FUN), USE.NAMES = FALSE)
    }
    xx <- sapply(1:length(xx), function(x) xx[[x]][1])
    
    # convert NULL to NA in list
    xx[sapply(xx, length) < 1] <- NA
    
    # return node values as a vector
    xx <- as.vector(unlist(xx))
    
    return(xx)
  }
  
  scraper_internal <- function(URL){
    print(URL)
    
    doc <- htmlParse(URL, encoding="UTF-8")
    
    #titles:
    title <- GS_xpathSApply(doc, "//div[@class='gs_ri']//h3[@class='gs_rt']", xmlValue)
    
    #types:
    #type = GS_xpathSApply(doc, "//h3//span//span[@class='gs_ct1']", xmlValue)
    
    #authors:
    author <-  GS_xpathSApply(doc, "//div[@class='gs_ri']//div[@class='gs_a']", xmlValue)
    
    #source:
    source  <-  GS_xpathSApply(doc, "//div[@class='gs_ggs gs_fl']//a[@href]", xmlValue)
    #print(source)
    #m.url <-  xpathSApply(doc, "//div[@class='gs_r']//div[@class='gs_ri']//h3[@class='gs_rt']", function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href')))
    #print(m.url) function(x) ifelse(x, NA, xmlGetAttr(x, 'href'))
    #o.url <- xpathSApply(doc, "//div[@class='gs_fl']//a[@class='gs_nph'][contains(., 'versions')]", function(x) ifelse(is.null(x), NA, xmlGetAttr(x, 'href')))
    #print(o.url)
    #d.url <-  xpathSApply(doc, "//div[@class='gs_r']//div[@class='gs_ggs gs_fl']", xmlValue)
    
    #links:
    mainLink <-  GS_xpathSApply(doc,  "//h3//a[@href]", xmlGetAttr)
    #print(mainLink)
    otherVersionLink <- GS_xpathSApply(doc, "//div[@class='gs_ri']//div[@class='gs_fl']//a[@class='gs_nph'][contains(., 'versions')]", xmlGetAttr)
    
    directLink <-  GS_xpathSApply(doc,  "//div[@class='gs_ggs gs_fl']//a[@href]", xmlGetAttr)
    #print(directLink)
    
    #cited by:
    cited.by = GS_xpathSApply(doc, "//div[@class='gs_ri']//div[@class='gs_fl']//a[contains(.,'Cited by')]/text()", xmlValue)
    
    #cited ref:
    #cited.ref = GS_xpathSApply(doc, "/div[@class='gs_ri']//div[@class='gs_fl']//a[contains(.,'Cited by')][@href]", xmlAttrs)
    
    
    #print(direct_url)
    
    #types:
    #type = GS_xpathSApply(doc, "//div[@class='gs_ggs gs_fl']//span[@class='gs_ctg2']", xmlValue)
    
    options(warn=(-1))
    dat <- data.frame(source = source,
                      title = title, 
                      #TYPE = type, 
                      authors = author,
                      year = as.integer(gsub(".*\\s(\\d{4})\\s.*", "\\1", author)), 
                      ncites = as.integer(as.integer(gsub("[^0-9]", "", cited.by))), 
                      #CITED_REF = cited.ref,
                      mainLink = mainLink,
                      otherVersionLink = otherVersionLink,
                      directLink = directLink,
                      stringsAsFactors = FALSE)
    
    options(warn=0)
    dat$source <- gsub(" ", "", gsub(".*\\]", "", dat$source))
    dat$title <- gsub(".*\\]", "", dat$title)
    #dat$otherVersionLink <- gsub("#", NA, dat$otherVersionLink)
    
    #to be used in Rascal
    dat$source <- sapply(dat$source, function(x) ifelse (is.na(x), NA, paste("|http://",x,"|", sep = "")))
    dat$mainLink <- sapply(dat$mainLink, function(x) ifelse (is.na(x), NA, paste("|",x,"|", sep = "")))
    dat$otherVersionLink <- sapply(dat$otherVersionLink, function(x) ifelse (is.na(x), NA, paste("|https://scholar.google.com",x,"|", sep = "")))
    dat$directLink <- sapply(dat$directLink, function(x) ifelse (is.na(x), NA, paste("|",x,"|", sep = "")))
    
    
    free(doc)
    return(dat)
  }
  
  result <- do.call("rbind", lapply(URLs, scraper_internal))
  if (write == T) {
    con <- file("/Users/mbotto/Google_Scholar_Scraper/GScholar_Output.CSV", encoding = "UTF-8")
    write.csv(result, con,
              row.names = F)
  } 
  return(result)
  
}

```


###Scrape test 2
```{r}
input <- '"remote sensing" "water+quality" "lakes"'
df <- GScholar_Scraper(input, 1, 0, since = 2008, to = 2017)



df <- cbind(id = as.numeric(rownames(df)), downloaded = as.character(FALSE), df)
df_save <- df[c(-5:-7)]
con <- file("/Users/mbotto/Google_Scholar_Scraper/ProgressData.csv", encoding = "UTF-8")
write.csv(df_save, con, row.names = F, na = "")


##check url
readUrl <- function(url) {
  out <- tryCatch(
    {
      HEAD(url)
    },
    error = function(cond) {
      return(NA)
    },
    warning = function(cond) {
      return(NA)
    },
    finally = { 
    }
  )    
  return(out)
}
```
