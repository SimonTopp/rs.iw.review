---
title: "Remote Sensing of Inland Waters Bibliometrix Report"
author: Simon Topp 
date: May 25, 2018
output: html_document
editor_options: 
  chunk_output_type: console
---
## This pulls in the data from the scopus search queries and conducts bibliometric analysis with the help of the bibliometrix package by Aria and Cuccurullo.

# Install and load bibliometrix R-package
```{r load bibliometrix}
# Stable version from CRAN (Comprehensive R Archive Network)
# if you need to execute the code, remove # from the beginning of the next line

# install.packages("bibliometrix")


# Most updated version from GitHub
# if you need to execute the code, remove # from the beginning of the next lines

# install.packages("devtools")
# devtools::install_github("massimoaria/bibliometrix")

library(bibliometrix)
library(prettydoc)
library(rio)
library(tidyverse)
library(feather)
```


# Data Loading and Converting
```{r Data loading, warning=FALSE}
####### Load in the Scopus data.  
Lakes <- readFiles('in/lakesv3.bib') %>% scopus2df(.)
Rivers <- readFiles('in/riversv3.bib') %>% scopus2df(.)
Estuary.Delta <- readFiles('in/estdeltv3.bib') %>% scopus2df(.)
InlandWater <- readFiles('in/case2v3.bib') %>% scopus2df(.)

M <- Lakes %>% full_join(Rivers) %>% full_join(Estuary.Delta) %>% full_join(InlandWater) %>% distinct(TI,AU, .keep_all = T)

#Clean the Journal Names a little bit
unique(M$SO)[order(unique(M$SO))]

M$SO <- M$SO %>% gsub(pattern = '^J GEOPHYS RES$', replacement = "JOURNAL OF GEOPHYSICAL RESEARCH", x = .) %>%
  gsub(pattern = '^LAKES AND RESERVOIRS: RESEARCH AND MANAGEMENT$', replacement = "LAKES & RESERVOIRS: RESEARCH AND MANAGEMENT", x = .) %>%
  gsub(pattern = '^PHOTOGRAMMETR\\. ENG\\. REMOTE SENS\\.$|^PHOTOGRAMMETR\\. ENGNG REMOTE SENS\\.$|^PHOTOGRAMMETRIC ENGINEERING & REMOTE SENSING$', replacement = "PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING", x = .) %>%
  gsub(pattern = '^SCIENCE OF THE TOTAL ENVIRONMENT, THE$', replacement = "SCIENCE OF THE TOTAL ENVIRONMENT", x = .)

write_feather(M, 'out/ScopusMerge.feather')
```

# Descriptive Analysis

## Main findings about the collection
```{r Descriptive Analysis, echo=TRUE, comment=NA}
results <- biblioAnalysis(M)
summ <- summary(results, k=10, pause=F, width=130)
#plot(x=results, k=10, pause=F)
```

## Most Cited References
```{r Most cited references,  comment=NA}
CR <- citations(M, field = "article", sep = "; ")
cbind(CR$Cited[1:50])
```

## Summary Scopus Scrape Figures

```{r}
# Combine data and create label for each search term
Lakes <- Lakes %>% mutate(Lakes = 1)
Rivers <- Rivers %>% mutate(Rivers = 1)
InlandWater <- InlandWater %>% mutate(InlandWater = 1)
Estuary.Delta <- Estuary.Delta %>% mutate(Estuaries = 1)
scrape <- Lakes %>% full_join(Rivers) %>% full_join(InlandWater) %>%full_join(Estuary.Delta) %>%
  mutate(Lakes = ifelse(is.na(Lakes), 0, Lakes),
         Rivers = ifelse(is.na(Rivers), 0, Rivers),
         InlandWater = ifelse(is.na(InlandWater),0,InlandWater),
         Estuaries = ifelse(is.na(Estuaries), 0, Estuaries)) %>%
  distinct(TI, AU, .keep_all = T)

scrape <- scrape %>%
  mutate(Waterbody = ifelse(Estuaries == 1, 'Estuaries and Deltas',ifelse(Rivers == 1, 'Rivers', ifelse(Lakes == 1, 'Lakes', 'Inland Waters')))) %>%
  mutate(Waterbody = ifelse(Lakes == 1 & Rivers == 1, 'Lakes and Rivers', Waterbody)) %>% distinct(TI, SO, .keep_all = T)


## Pull Most Cited Papers from Bibliometrix results
cites <- as.tibble(summ$MostCitedPapers)

##Pull Country and Citation Production out
countries <- factor(summ$MostProdCountries$`Country  `)
count <- as.numeric(summ$MostProdCountries$Articles)
SCP <- as.numeric(summ$MostProdCountries$SCP)
MCP <- as.numeric(summ$MostProdCountries$MCP)
country10 <- bind_cols(Country = countries,Total = count, SCP = SCP, MCP = MCP) %>% gather(Type, Publications, SCP:MCP)
country10$Country <- reorder(country10$Country, country10$Publications)


## Pull most relevant sources
sources <- factor(summ$MostRelSources$`Sources       `)
count <- as.numeric(summ$MostRelSources$Articles)
Source10 <- bind_cols(Journal = sources,Publications = count)

Source10$Journal <- reorder(Source10$Journal, Source10$Publications)


## Publications Per Journal and Country (Figure 2)
p1 <- ggplot(country10, aes(x = Country, y = Publications, fill = Type)) +
  geom_col() + 
  scale_x_discrete(labels = c('Finland', 'Germany', 'Brazil', 'Australia', 'Canada', 'Italy', 'Netherlands', 'India', 'China', 'USA')) +
  scale_fill_brewer(palette = 'Paired', labels = c('Multicountry', 'Single\nCountry')) +
  theme_bw() +
  labs(fill = 'Country\nCollaboration') +
  theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 0.5),
        legend.title = element_text(size = 9),
        legend.text = element_text(size = 8))#,
        #axis.title.x = element_text(margin = margin(t = 30, b = 25)))

p2 <- ggplot(Source10, aes(x = Journal, y = Publications)) +
  geom_col() + theme_bw() + 
  scale_x_discrete(labels = c('Spectroscopy and \nSpectral Analysis (China)', 'J. of Env. Management', 'J. of Great\nLakes Research', 'Environmental Science\n(China)', 'Photogr. Eng. and\nRemote Sensing','Remote Sensing', 'Sci. of the Total\nEnvironment', 'Env. Monitoring\n and Assessment', 'Int. Journal of \nRemote Sensing', 'Remote Sensing\nof Environment')) +
  theme(axis.text.x = element_text(size = 8, angle = 90, hjust = 1, vjust = 0.5))#,
       # axis.title.y = element_blank())


g <- grid.arrange(p1, p2, nrow = 2, heights = c(1,1.3))

ggsave(filename = 'figures/CountryJournal2Panel.png', plot = g, height = 6, width = 3.5, unit = 'in', dpi = 250)
```


## Average Citations per Year

```{r}
# This averages total citations over the number of years an article has been available and integrates all of them to create an estimate of total citations per year.
x <- tibble(Year = results$Years, TotalCitation = results$TotalCitation) %>%
  group_by(Year) %>%
  dplyr::summarise(count = n(),
                   TotalCitations = sum(TotalCitation),
                   meanCitesPerPub = mean(TotalCitation)) %>%
  mutate(AgeYears = 2018.75 - Year,
         CitesPerYear = TotalCitations/AgeYears)

## Sum average yearly sites over number of years
for (i in 1:length(x$Year)){
  x$yearly.cites[i] <- sum(x$CitesPerYear[1:i])
}

scrape <- scrape %>% 
  mutate(Year = as.numeric(PY)) %>%
  left_join(x, by = c('Year'))


# Plot the total publication counts and average citations (Figrure 1)
ggplot(scrape) + 
  geom_bar(aes(x = Year, y = ..count.., fill = Waterbody)) +
  geom_line(aes(x = Year, y = yearly.cites/16), color = 'Red') +
  scale_fill_manual(values = c('#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fdbf6f'), labels = c('Estuaries\nand Deltas', 'Inland\nWaters','Lakes', 'Lakes and\nRivers', 'Rivers')) +
  labs(y = 'Publication Count', x = 'Year') +
  scale_x_continuous(limits = c(1970, 2018), expand = c(0,0)) +
  scale_y_continuous(limits = c(0,125), expand =c(0,0),
                     sec.axis = sec_axis(~.*16, name  = 'Average Citation Count')) + theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.text.y.right = element_text(color = "red"),
        axis.title.y.right = element_text(color = 'red', margin = margin(l = 10)),
        legend.position = 'bottom',
        legend.title = element_blank(),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(l = -10,r = -10,t =-10,b =20))

ggsave('figures/PubCountsYearly.png', width = 4, height = 4, unit = 'in', dpi = 300)

trend <- TcYr %>% filter(N != 0)

##Fit a trend line to the data
# Linear
lm <- lm(x$count~x$Year)
# Log
lm2 <- lm(log(x$count)~x$Year)
summary(lm2)
## Calculate doubling time
log(2)/lm2$coefficients[2]

ggplot(x) + geom_point(aes(x = Year, y = exp(lm2$fitted.values))) + geom_point(aes(x = Year, y = count), colour = 'red')

# Average Annual Increase
(x$count[x$Year == max(x$Year)]/x$count[x$Year == min(x$Year)])^(1/(max(x$Year) - min(x$Year))) - 1


#Check 1980-2012 to compare to Bornman and Mutz 2015
check <- x %>% filter(Year >1979, Year <2013)
(x$count[x$Year == 2012]/x$count[x$Year == 1980])^(1/33) - 1
lmcheck<- lm(log(check$count)~check$Year)
#Doubling time
log(2)/lmcheck$coefficients[2]



# Compare publication counts pre and post 2008
x$resid <- lm2$residuals
t1 <- x %>% filter(Year > 2002 & Year < 2008)
t2 <- x %>% filter(Year > 2008 & Year < 2014)

t.test(t1$count, t2$count)
t.test(t2$resid, t1$resid)

```

## Word Cloud Experiment (Not used)
```{r}
# Load
library(tm)
library(SnowballC)
library(wordcloud)


keywords <- write(scrape$Author.Keywords, 'out/scrapeKeyWords.txt')
keys <- readLines('out/scrapeKeyWords.txt')
docs <- Corpus(VectorSource(keys))
inspect(docs)

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
inspect(docs)

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
d <- d %>% filter(freq < 1000)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 5,
          max.words=200, random.order=T, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
wordcloud

x = termExtraction(M, Field = 'ID')
terms$terms <- x$ID_TM %>% str_split(., pattern = ';') %>% unlist() %>% group_by(terms) %>% summarize(count = n())
terms = bind_cols(terms, rep(1, 51620))
```


# The remaining code isn't used in the manuscript but is interesting to play around with.  It's pulled from the example script in the bibliometrixs git.

## Network Analysis of main relational dimensions

### Co-citation Analysis: the Intellectual Structure of the field

**Plot options**:

* n = 50 (the funxtion plots the main 50 cited references)

* type = "fruchterman" (the network layout is generated using the Fruchterman-Reingold Algorithm)

* size.cex = TRUE (the size of the vertices is proportional to their degree)

* size = 20 (the max size of vertices)

* remove.multiple=FALSE (multiple edges are not removed)

* labelsize = 0.7 (defines the size of vertex labels)

* edgesize = 10 (The thickness of the edges is proportional to their strength. Edgesize defines the max value of the thickness)

* edges.min = 5 (plots only edges with a strength greater than or equal to 5)

* all other arguments assume the default values

```{r Co-citation network, comment=NA, eval = F}
# Attempt to clean references first
M$CR <-  str_replace_all(pattern = ";\\s\\(\\d+\\)", string = M$CR, replacement = '; ')

# Create co-citation network
NetMatrix <- biblioNetwork(M, analysis = "co-citation", network = "references", sep = ";")
png(filename='figures/CoCite.png', width=3.5,height=4, units='in', res=250)
net=networkPlot(NetMatrix, n = 40, Title = "Co-Citation Network", type = "fruchterman",vos.path = '/Applications/VOSviewer_1.6.8_jar', size.cex=TRUE, size=10, remove.multiple=T, labelsize=0.7,edgesize = 3, edges.min=5, label.n = 25)
dev.off()

NetMatrix <- biblioNetwork(M, analysis = "co-citation", network = "sources", sep = ";")

net=networkPlot(NetMatrix, n = 30, Title = "Co-Citation Network", type = "fruchterman",vos.path = '/Applications/VOSviewer_1.6.8_jar', size.cex=TRUE, size=10, remove.multiple=T, labelsize=0.7,edgesize = 3, edges.min=5, label.n = 25)

check <- M$JI %>% str_split(pattern = '; ', string = .)

M <- metaTagExtraction(M, Field = 'CR_JI', sep = ';')
authorsColab <- biblioNetwork(N, analysis = "co-citation", network = "authors", sep = ';') #"^; \\w$")
net=networkPlot(authorsColab, normalize = 'association', n = 50, Title = "Author Collaboration Network", type = "kamada", size.cex=TRUE, size=10, remove.multiple=T, labelsize=0.7,edgesize = 10, edges.min=5)
?networkPlot
```


Descriptive analysis of co-citation network characteristics

```{r Co-citation net stat, comment=NA, , eval = F}
netstat <- networkStat(net$graph)
summary(netstat,k=10)
```


## Keyword co-occurrences network

**Plot options**:

* normalize = "association" (the vertex similarities are normalized using association strength)

* n = 50 (the function plots the main 50 cited references)

* type = "fruchterman" (the network layout is generated using the Fruchterman-Reingold Algorithm)

* size.cex = TRUE (the size of the vertices is proportional to their degree)

* size = 20 (the max size of the vertices) 

* remove.multiple=FALSE (multiple edges are not removed)

* labelsize = 3 (defines the max size of vertex labels)

* label.cex = TRUE (The vertex label sizes are proportional to their degree)

* edgesize = 10 (The thickness of the edges is proportional to their strength. Edgesize defines the max value of the thickness)

* label.n = 30 (Labels are plotted only for the main 30 vertices)

* edges.min = 25 (plots only edges with a strength greater than or equal to 2)

* all other arguments assume the default values

```{r Keyword co-occurrences, comment=NA, fig.height=10, fig.width=10, eval = F}
NetMatrix <- biblioNetwork(M, analysis = "co-citation", network = "references", sep = ";")

net=networkPlot(NetMatrix, normalize="association", n = 50, Title = "Keyword Co-occurrences", type = "kamada", size.cex=TRUE, size=20, remove.multiple=F, edgesize = 3, labelsize=3,label.cex=TRUE,label.n=30,edges.min=2)
```

Descriptive analysis of keyword co-occurrences network characteristics

```{r Keyword net stat, comment=NA, eval = F}
netstat <- networkStat(NetMatrix)
summary(netstat,k=10)
```


## Author collaboration network
```{r, Au collaboration network, fig.height=10, fig.width=10, eval = F}
NetMatrix <- biblioNetwork(M, analysis = "collaboration",  network = "authors", sep = ";")
net=networkPlot(NetMatrix,  n = 50, Title = "Author collaboration",type = "kamada", size=3,size.cex=T,edgesize = 1,labelsize=0.6, remove.isolates = T)
```

Descriptive analysis of author collaboration network characteristics

```{r Au coll stat, comment=NA, eval = F}
netstat <- networkStat(NetMatrix)
summary(netstat,k=15)
```


## Edu collaboration network
```{r, Edu collaboration network, fig.height=10, fig.width=10, eval = F}
M <- metaTagExtraction(M, Field = 'AU_UN', sep = ';')
NetMatrix <- biblioNetwork(M, analysis = "collaboration",  network = "universities", sep = ";")
net=networkPlot(NetMatrix,  n = 50, Title = "Edu collaboration",type = "auto", size=10,size.cex=T,edgesize = 3,labelsize=0.6)
```

Descriptive analysis of edu collaboration network characteristics

```{r Edu coll stat, comment=NA, eval = F}
netstat <- networkStat(NetMatrix)
summary(netstat,k=15)
```


## Co-Word Analysis: The conceptual structure of the field
```{r Co-word Analysis, fig.height=10, fig.width=10, eval = F}
CS <- conceptualStructure(M, method="CA", field="ID", minDegree=10, k.max = 8, stemming=f, labelsize=8,documents=20)
```

## Historiograph

```{r Direct citation network, fig.height=10, fig.width=10, eval = F}
histResults <- histNetwork(M, sep = ";")

```

```{r Historiograph, comment=NA, fig.height=7,fig.width=10, eval = F}
options(width = 130)
net <- histPlot(histResults, n=20, size.cex=TRUE, size = 5, labelsize = 3, arrowsize = 0.5)
```


## Thematic Map

```{r Keyword Network, include=FALSE, eval = F}
NetMatrix <- biblioNetwork(M, analysis = "co-occurrences",network = "keywords", sep = ";")
S <- normalizeSimilarity(NetMatrix, type = "association")
net1 <- networkPlot(S, n=500, Title = "Keyword co-occurrences",type="fruchterman",
                   labelsize = 2, halo = F, cluster = "walktrap",remove.isolates=FALSE,
                   remove.multiple=FALSE, noloops=TRUE, weighted=TRUE,label.cex=T,edgesize=5, size=1,edges.min = 2)
```


```{r ThematicMap, echo=FALSE, fig.height=9, fig.width=9, eval = F}

Map=thematicMap(net1, NetMatrix, S = S,minfreq=3)
plot(Map$map)
```
